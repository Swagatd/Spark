{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Introduction .ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"9F5fRTwXNlP8","colab_type":"text"},"cell_type":"markdown","source":["###What is Apache Spark\n","\n","Spark is an open source, scalable, massively parallel, in-memory execution environment for running analytics applications. Think of it as an in-memory layer that sits above multiple data stores, where data can be loaded into memory and analyzed in parallel across a cluster.\n","\n","Spark also includes prebuilt machine-learning algorithms and graph analysis algorithms that are especially written to execute in parallel and in memory. It also supports interactive SQL processing of queries and real-time streaming analytics.\n","As a result, you can write analytics applications in programming languages such as Java, Python, R and Scala.\n","\n","\n","\n","###Advatage of Apache Spark:\n","\n","#####Speed - \n","Spark is capable of performing batch, interactive and Machine Learning and Streaming all in the same cluster. As a result makes it a completedata analytics engine.\n","\n","\n","#####Easy to Manage -\n","Spark is lightning fast cluster computing tool. Apache Spark runs applications up to 100x faster in memory and 10x faster on disk than Hadoop. Because of reducing the number of read/write cycle to disk and storing intermediate data in-memory Spark makes it possible.\n","\n","\n","#####Supports Real time and Batch processing -\n","Apache Spark supports “Batch data” processing where a group of transactions is collected over a period of time. It also supports real time data processing, where data is continuously flowing from the source. For example, weather information coming in from sensors can be processed by Apache Spark directly.\n","\n","#####Language Support - \n","Apache Spark has API support for popular data science languages like Python, R, Scala and Java.\n","\n","###Spark Data API:\n","\n","####Resilient Distributed Dataset (RDD)\n","\n","RDD is an immutable distributed collection of elements of your data, partitioned across nodes in your cluster that can be operated in parallel with a low-level API that offers transformations and actions.\n","\n","**When to use RDDs?**\n","\n","\n","*   you want low-level transformation and actions and control on your dataset\n","*   your data is unstructured, such as media streams or streams of text\n","*  you don’t care about imposing a schema, such as columnar format, while processing or accessing data attributes by name or column\n","\n","####DataFrames\n","DataFrame is an immutable distributed collection of data. Unlike an RDD, data is organized into named columns, like a table in a relational database. Designed to make large data sets processing even easier.\n","\n","#####DataSet\n","\n","Starting in Spark 2.0, Dataset takes on two distinct APIs characteristics: a strongly-typed API and an untyped API, as shown in the table below. Conceptually, consider DataFrame as an alias for a collection of generic objects Dataset[Row], where a Row is a generic untyped JVM object. Dataset, by contrast, is a collection of strongly-typed JVM objects, dictated by a case class you define in Scala or a class in Java.\n"]}]}